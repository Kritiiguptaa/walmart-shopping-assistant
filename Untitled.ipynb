{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e32aa0d6-5196-4b3d-9543-740762e3988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Walmart dataset...\n",
      "‚úÖ Cleaned data saved as useful1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Load data\n",
    "print(\"üì• Loading Walmart dataset...\")\n",
    "df = pd.read_csv(r\"C:\\Users\\kriti\\Desktop\\walmart-chatbot\\dataset_1.csv\")\n",
    "\n",
    "# Step 2: Select useful columns\n",
    "useful_cols = [\n",
    "    \"Product Name\", \"Product Url\", \"Description\", \"Category\", \n",
    "    \"Sale Price\", \"List Price\", \"Brand\", \"Available\"\n",
    "]\n",
    "clean_df = df[useful_cols].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Step 3: Clean 'Available' column to True/False\n",
    "def clean_available(val):\n",
    "    val = str(val).strip().lower()\n",
    "    return val in [\"true\", \"1\"]\n",
    "\n",
    "clean_df[\"Available\"] = clean_df[\"Available\"].apply(clean_available)\n",
    "\n",
    "# Step 4: Remove disclaimer from 'Description'\n",
    "disclaimer_pattern = (\n",
    "    r\"We aim to show you accurate product information\\. Manufacturers, suppliers and others \"\n",
    "    r\"provide what you see here, and we have not verified it\\. See our disclaimer \\|.*?\"\n",
    ")\n",
    "\n",
    "clean_df[\"Description\"] = clean_df[\"Description\"].str.replace(\n",
    "    disclaimer_pattern, \"\", regex=True\n",
    ").str.strip()\n",
    "\n",
    "# Step 5: Clean 'Category' format\n",
    "clean_df[\"Category\"] = clean_df[\"Category\"].str.replace(\"|\", \">\", regex=False).str.strip()\n",
    "\n",
    "# Step 6: Convert prices to numeric\n",
    "clean_df[\"Sale Price\"] = pd.to_numeric(clean_df[\"Sale Price\"], errors=\"coerce\")\n",
    "clean_df[\"List Price\"] = pd.to_numeric(clean_df[\"List Price\"], errors=\"coerce\")\n",
    "\n",
    "# Step 7: Fill missing values\n",
    "clean_df = clean_df.fillna({\n",
    "    \"Sale Price\": 0,\n",
    "    \"List Price\": 0,\n",
    "    \"Brand\": \"Unknown\",\n",
    "    \"Description\": \"\",\n",
    "    \"Category\": \"Uncategorized\",\n",
    "    \"Available\": False\n",
    "})\n",
    "\n",
    "# Step 8: Truncate overly long descriptions\n",
    "clean_df[\"Description\"] = clean_df[\"Description\"].apply(lambda x: x[:1000])\n",
    "\n",
    "# Step 9: Calculate Discount (%)\n",
    "clean_df[\"Discount (%)\"] = (\n",
    "    (clean_df[\"List Price\"] - clean_df[\"Sale Price\"]) / clean_df[\"List Price\"] * 100\n",
    ").round(2)\n",
    "clean_df[\"Discount (%)\"] = clean_df[\"Discount (%)\"].fillna(0)\n",
    "\n",
    "# Step 10: Rename columns\n",
    "clean_df.rename(columns={\n",
    "    \"Product Name\": \"product_name\",\n",
    "    \"Product Url\": \"product_url\",\n",
    "    \"Description\": \"description\",\n",
    "    \"Sale Price\": \"sale_price\",\n",
    "    \"List Price\": \"list_price\",\n",
    "    \"Brand\": \"brand\",\n",
    "    \"Category\": \"category\",\n",
    "    \"Available\": \"is_available\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 11: Save cleaned data\n",
    "output_path = r\"C:\\Users\\kriti\\Desktop\\walmart-chatbot\\useful1.csv\"\n",
    "\n",
    "# Check if file is already open or locked\n",
    "try:\n",
    "    clean_df.to_csv(output_path, index=False)\n",
    "    print(\"‚úÖ Cleaned data saved as useful1.csv\")\n",
    "except PermissionError:\n",
    "    print(\"‚ùå ERROR: File is open in another program. Please close it and run again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa4ef66-1c4a-495d-888c-fb75ae5d7aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Walmart dataset...\n",
      "‚úÖ Cleaned data saved as useful1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Load data\n",
    "print(\"üì• Loading Walmart dataset...\")\n",
    "df = pd.read_csv(r\"C:\\Users\\kriti\\Desktop\\walmart-chatbot\\dataset_2.csv\")\n",
    "\n",
    "# Step 2: Select useful columns\n",
    "useful_cols = [\n",
    "    \"Product Name\", \"Product Url\", \"Description\", \"Category\", \n",
    "    \"Sale Price\", \"List Price\", \"Brand\", \"Available\"\n",
    "]\n",
    "clean_df = df[useful_cols].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Step 3: Clean 'Available' column to True/False\n",
    "def clean_available(val):\n",
    "    val = str(val).strip().lower()\n",
    "    return val in [\"true\", \"1\"]\n",
    "\n",
    "clean_df[\"Available\"] = clean_df[\"Available\"].apply(clean_available)\n",
    "\n",
    "# Step 4: Remove disclaimer from 'Description'\n",
    "disclaimer_pattern = (\n",
    "    r\"We aim to show you accurate product information\\. Manufacturers, suppliers and others \"\n",
    "    r\"provide what you see here, and we have not verified it\\. See our disclaimer \\|.*?\"\n",
    ")\n",
    "\n",
    "clean_df[\"Description\"] = clean_df[\"Description\"].str.replace(\n",
    "    disclaimer_pattern, \"\", regex=True\n",
    ").str.strip()\n",
    "\n",
    "# Step 5: Clean 'Category' format\n",
    "clean_df[\"Category\"] = clean_df[\"Category\"].str.replace(\"|\", \">\", regex=False).str.strip()\n",
    "\n",
    "# Step 6: Convert prices to numeric\n",
    "clean_df[\"Sale Price\"] = pd.to_numeric(clean_df[\"Sale Price\"], errors=\"coerce\")\n",
    "clean_df[\"List Price\"] = pd.to_numeric(clean_df[\"List Price\"], errors=\"coerce\")\n",
    "\n",
    "# Step 7: Fill missing values\n",
    "clean_df = clean_df.fillna({\n",
    "    \"Sale Price\": 0,\n",
    "    \"List Price\": 0,\n",
    "    \"Brand\": \"Unknown\",\n",
    "    \"Description\": \"\",\n",
    "    \"Category\": \"Uncategorized\",\n",
    "    \"Available\": False\n",
    "})\n",
    "\n",
    "# Step 8: Truncate overly long descriptions\n",
    "clean_df[\"Description\"] = clean_df[\"Description\"].apply(lambda x: x[:1000])\n",
    "\n",
    "# Step 9: Calculate Discount (%)\n",
    "clean_df[\"Discount (%)\"] = (\n",
    "    (clean_df[\"List Price\"] - clean_df[\"Sale Price\"]) / clean_df[\"List Price\"] * 100\n",
    ").round(2)\n",
    "clean_df[\"Discount (%)\"] = clean_df[\"Discount (%)\"].fillna(0)\n",
    "\n",
    "# Step 10: Rename columns\n",
    "clean_df.rename(columns={\n",
    "    \"Product Name\": \"product_name\",\n",
    "    \"Product Url\": \"product_url\",\n",
    "    \"Description\": \"description\",\n",
    "    \"Sale Price\": \"sale_price\",\n",
    "    \"List Price\": \"list_price\",\n",
    "    \"Brand\": \"brand\",\n",
    "    \"Category\": \"category\",\n",
    "    \"Available\": \"is_available\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 11: Save cleaned data\n",
    "output_path = r\"C:\\Users\\kriti\\Desktop\\walmart-chatbot\\useful2.csv\"\n",
    "\n",
    "# Check if file is already open or locked\n",
    "try:\n",
    "    clean_df.to_csv(output_path, index=False)\n",
    "    print(\"‚úÖ Cleaned data saved as useful2.csv\")\n",
    "except PermissionError:\n",
    "    print(\"‚ùå ERROR: File is open in another program. Please close it and run again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7c13e20-ef25-486e-842d-eefd3a3e5985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          product_name  \\\n",
      "1    Exultantex Grey Blackout Curtains for Living R...   \n",
      "62   Amay Grommet Top Blackout Curtain Panel White ...   \n",
      "97   Amay Blackout Grommet Curtain Panel Beige 42 I...   \n",
      "107  Amay Grommet Top Blackout Curtain Panel Greyis...   \n",
      "141  Amay Rod Pocket Window Curtain Panel Silver Gr...   \n",
      "144  Amay Grommet Top Blackout Curtain Panel Greyis...   \n",
      "165  Amay Blackout Double Pinch Pleat Curtain Panel...   \n",
      "182  Amay Rod Pocket Curtain Panel Draperies Ivory ...   \n",
      "235  Mainstays 4 of a Kind Blackout Curtain Panels,...   \n",
      "266  Amay Grommet Blackout Curtain Panel True Red 8...   \n",
      "375  Amay Rod Pocket Curtain Panel Yellow/Gold 120 ...   \n",
      "442  Amay Blackout Double Pinch Pleat Curtain Panel...   \n",
      "463  Amay Rod Pocket Curtain Panel Draperies Silver...   \n",
      "482  Mainstays Blackout Curtain Panel Pair, Set of ...   \n",
      "516  Amay Blackout Double Pinch Pleat Curtain Panel...   \n",
      "533  Amay Blackout Grommet Curtain Panel Taupe 52 I...   \n",
      "554  50 x 84 in. Tranquil Lined Grommet Window Curt...   \n",
      "592  Amay Rod Pocket Window Curtain Panel Burgundy ...   \n",
      "641  Eclipse Dayton Solid Blackout Rod Pocket Energ...   \n",
      "709  Amay Rod Pocket Window Curtain Panel Grey 84 I...   \n",
      "757  Amay Grommet Blackout Curtain Panel Baby Pink ...   \n",
      "794  Amay Rod Pocket Curtain Panel Draperies Beige ...   \n",
      "800  Amay Rod Pocket Curtain Panel Baby Pink 120 In...   \n",
      "833  Amay Grommet Top Blackout Curtain Panel Greyis...   \n",
      "853       BrylaneHome Pre-Lit Rod-Pocket Curtain Panel   \n",
      "914  Amay Rod Pocket Window Curtain Panel Black 84 ...   \n",
      "922  Amay Grommet Top Blackout Curtain Panel Greyis...   \n",
      "930  Amay Blackout Double Pinch Pleat Curtain Panel...   \n",
      "973  Amay Grommet Top Blackout Curtain Panel Grey 6...   \n",
      "\n",
      "                                                colors  \n",
      "1                Black, Blue, Gray, Green, Ivory, Pink  \n",
      "62   Beige, Black, Blue, Brown, Burgundy, Gold, Gra...  \n",
      "97   Beige, Black, Blue, Brown, Burgundy, Gold, Gra...  \n",
      "107  Beige, Black, Blue, Brown, Burgundy, Gold, Gra...  \n",
      "141  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "144  Beige, Black, Blue, Brown, Burgundy, Gold, Gra...  \n",
      "165  Beige, Black, Blue, Burgundy, Chocolate, Gray,...  \n",
      "182  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "235  Black, Blue, Dark Blue, Gray, Green, Navy, Sil...  \n",
      "266  Beige, Black, Blue, Brown, Burgundy, Gold, Gra...  \n",
      "375  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "442  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "463  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "482  Beige, Black, Blue, Burgundy, Gold, Gray, Grey...  \n",
      "516  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "533  Beige, Black, Blue, Brown, Burgundy, Gold, Gra...  \n",
      "554                                 Green, Silver, Tan  \n",
      "592  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "641                Black, Chocolate, Green, Ivory, Red  \n",
      "709  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "757  Beige, Black, Blue, Brown, Burgundy, Gold, Gra...  \n",
      "794  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "800  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "833  Beige, Black, Blue, Brown, Burgundy, Gold, Gra...  \n",
      "853                              Burgundy, Gold, White  \n",
      "914  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "922  Beige, Black, Blue, Brown, Burgundy, Gold, Gra...  \n",
      "930  Beige, Black, Blue, Brown, Burgundy, Gray, Gre...  \n",
      "973  Beige, Black, Blue, Brown, Burgundy, Gold, Gra...  \n",
      "‚úÖ Cleaned dataset saved as 'cleaned_walmart_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\kriti\\Desktop\\walmart-chatbot\\walmart-products.csv\")\n",
    "\n",
    "# --- Helper functions ---\n",
    "def parse_json_column(val):\n",
    "    try:\n",
    "        return json.loads(val.replace(\"'\", '\"')) if isinstance(val, str) else val\n",
    "    except:\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"walmart-products.csv\")  # Use your actual CSV path\n",
    "\n",
    "# ‚úÖ Define standard sizes\n",
    "standard_sizes = set([\n",
    "    # Clothing sizes\n",
    "    \"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\", \"XXXL\",\n",
    "\n",
    "    # Kids / baby sizes\n",
    "    \"0-3 Months\", \"3-6 Months\", \"6-12 Months\", \"12 Months\", \"18 Months\",\n",
    "    \"2T\", \"3T\", \"4T\", \"5T\", \"6T\",\n",
    "\n",
    "    # Shoe / numeric\n",
    "    \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",\n",
    "\n",
    "    # Dimensional\n",
    "    \"50 X 54\", \"50 X 63\", \"50 X 84\", \"50 X 95\", \"52 X 84\", \"42 X 84\",\n",
    "    \"84 X 95\", \"84 X 96\", \"108 X 84\", \"63 X 95\", \"84 X 84\",\n",
    "\n",
    "    # Mattress\n",
    "    \"Twin\", \"Twin XL\", \"Full\", \"Queen\", \"King\", \"California King\",\n",
    "\n",
    "    # Misc\n",
    "    \"Standard\", \"One Size\", \"Plus Size\"\n",
    "])\n",
    "\n",
    "# üîç Size cleaner function\n",
    "def extract_valid_standard_sizes(size_string):\n",
    "    if pd.isna(size_string):\n",
    "        return \"\"\n",
    "\n",
    "    parts = re.split(r\"[,\\|/]+\", str(size_string))\n",
    "    valid_sizes = []\n",
    "\n",
    "    for s in parts:\n",
    "        s = s.strip().title()\n",
    "        if s in standard_sizes:\n",
    "            valid_sizes.append(s)\n",
    "        elif re.match(r\"^\\d{2,3}\\s?[xX]\\s?\\d{2,3}$\", s):  # e.g., 84 x 95\n",
    "            valid_sizes.append(s.replace(\"x\", \"X\").replace(\" \", \"\"))  # normalize format like 84X95\n",
    "\n",
    "    return \", \".join(sorted(set(valid_sizes)))\n",
    "\n",
    "# def extract_sizes(val):\n",
    "#     if isinstance(val, str):\n",
    "#         return \", \".join(ast.literal_eval(val))\n",
    "#     return val\n",
    "# üìè Clean and extract standardized sizes\n",
    "\n",
    "\n",
    "# all_sizes = set()\n",
    "# product_df[\"sizes\"].dropna().apply(lambda val: all_sizes.update(extract_valid_sizes(val)))\n",
    "# selected_sizes = st.sidebar.multiselect(\"üìè Select Sizes\", sorted(all_sizes))\n",
    "\n",
    "# def extract_colors(val):\n",
    "#     if pd.isna(val):\n",
    "#         return \"\"\n",
    "#     try:\n",
    "#         if isinstance(val, str):\n",
    "#             parsed = ast.literal_eval(val) if val.startswith(\"[\") else val\n",
    "#             if isinstance(parsed, list):\n",
    "#                 return \", \".join([c.strip().title() for c in parsed])\n",
    "#             else:\n",
    "#                 return str(parsed).title()\n",
    "#     except:\n",
    "#         return str(val).title()\n",
    "#     return val\n",
    "standard_colors = {\n",
    "    \"Black\", \"White\", \"Red\", \"Green\", \"Blue\", \"Yellow\", \"Purple\", \"Orange\",\n",
    "    \"Gray\", \"Grey\", \"Pink\", \"Brown\", \"Beige\", \"Teal\", \"Navy\", \"Maroon\",\n",
    "    \"Olive\", \"Turquoise\", \"Lavender\", \"Coral\", \"Gold\", \"Silver\", \"Ivory\",\n",
    "    \"Cyan\", \"Magenta\", \"Indigo\", \"Mint\", \"Peach\", \"Tan\", \"Chocolate\",\n",
    "    \"Copper\", \"Burgundy\", \"Plum\", \"Rose\", \"Lilac\", \"Mauve\", \"Rust\", \n",
    "    \"Charcoal\", \"Mustard\", \"Denim\", \"Khaki\", \"Cream\", \"Sky Blue\", \n",
    "    \"Dark Blue\", \"Light Blue\", \"Light Green\", \"Dark Green\", \n",
    "    \"Dark Gray\", \"Light Gray\", \"Hot Pink\", \"Slate\", \"Lime\", \n",
    "    \"Aqua\", \"Sand\", \"Wine\", \"Amber\"\n",
    "}\n",
    "\n",
    "# üéØ Color cleaner to match only standard colors\n",
    "def clean_colors(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    tokens = re.split(r\",|\\+|\\/|\\-|:|\\\\n|\\\\t\", str(text))\n",
    "    colors = set()\n",
    "\n",
    "    for token in tokens:\n",
    "        token_clean = re.sub(r'^[^a-zA-Z]+', '', token).strip().title()\n",
    "        for standard in standard_colors:\n",
    "            if standard.lower() in token_clean.lower():\n",
    "                colors.add(standard)\n",
    "                break\n",
    "\n",
    "    return \", \".join(sorted(colors))\n",
    "\n",
    "\n",
    "# def clean_colors(text):\n",
    "#     if pd.isna(text):\n",
    "#         return []\n",
    "\n",
    "#     tokens = re.split(r\",|\\+|\\/|-|:\", text)  # Split on multiple delimiters\n",
    "#     clean_colors = set()\n",
    "\n",
    "#     for token in tokens:\n",
    "#         # Remove all non-letter characters at the beginning\n",
    "#         token = re.sub(r'^[^a-zA-Z]+', '', token).strip()\n",
    "\n",
    "#         # Only keep if it contains letters and is reasonable in length\n",
    "#         if re.search(r'[a-zA-Z]', token) and 2 <= len(token) <= 25:\n",
    "#             clean_colors.add(token.title())\n",
    "\n",
    "#     return sorted(clean_colors)\n",
    "\n",
    "\n",
    "def clean_free_returns(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    return str(val).strip()\n",
    "\n",
    "\n",
    "def clean_text(val):\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    return str(val).replace(\"\\n\", \" \").replace(\"√¢‚ÄìÀÜ\", \"\").strip()\n",
    "\n",
    "def extract_category_url(breadcrumbs):\n",
    "    try:\n",
    "        items = parse_json_column(breadcrumbs)\n",
    "        if isinstance(items, list) and items:\n",
    "            return items[-1].get(\"url\")\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# def extract_main_image(val):\n",
    "#     if pd.isna(val):\n",
    "#         return None\n",
    "#     try:\n",
    "#         urls = ast.literal_eval(val)  # Converts string list to Python list\n",
    "#         if isinstance(urls, list) and urls:\n",
    "#             return urls[0]  # Return the first image URL\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     return None\n",
    "def clean_main_image(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    return str(val).strip().strip('\"')  # Remove extra quotes if present\n",
    "\n",
    "\n",
    "# --- Clean & transform columns ---\n",
    "df[\"product_name\"] = df[\"product_name\"].apply(clean_text)\n",
    "df[\"description\"] = df[\"description\"].apply(clean_text)\n",
    "# df[\"discount\"] = df[\"discount\"].fillna(0).astype(str).str.replace(\"null\", \"0\").astype(float)\n",
    "def clean_price(val):\n",
    "    if pd.isna(val):\n",
    "        return 0.0\n",
    "    val = str(val).replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "    try:\n",
    "        return float(val)\n",
    "    except:\n",
    "        return 0.0  # fallback if not convertible\n",
    "\n",
    "df[\"discount\"] = df[\"discount\"].apply(clean_price)\n",
    "df[\"final_price\"] = df[\"final_price\"].apply(clean_price)\n",
    "\n",
    "df[\"sizes\"] = df[\"sizes\"].apply(extract_valid_standard_sizes)\n",
    "# df[\"colors\"] = df[\"colors\"].apply(lambda x: \", \".join(clean_colors(x)))\n",
    "df[\"colors\"] = df[\"colors\"].apply(clean_colors)\n",
    "df[\"free_returns\"] = df[\"free_returns\"].apply(clean_free_returns)\n",
    "df[\"category_url\"] = df[\"breadcrumbs\"].apply(extract_category_url)\n",
    "df[\"image_url\"] = df[\"main_image\"].apply(clean_main_image)\n",
    "# --- Select useful columns ---\n",
    "df.rename(columns={\"url\": \"product_url\"}, inplace=True)\n",
    "useful_columns = [\n",
    "    \"product_name\", \"description\", \"final_price\", \"discount\", \"rating\", \"review_count\",\n",
    "    \"sizes\", \"colors\", \"free_returns\", \"category_url\", \"product_url\", \"brand\", \"image_url\"\n",
    "]\n",
    "\n",
    "df_cleaned = df[useful_columns].copy()\n",
    "\n",
    "# --- Final cleanup ---\n",
    "# df_cleaned.drop_duplicates(inplace=True)\n",
    "# üîÑ Convert lists to strings before deduplication\n",
    "if isinstance(df_cleaned.loc[0, \"colors\"], list):\n",
    "    df_cleaned[\"colors\"] = df_cleaned[\"colors\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Now it's safe to drop duplicates\n",
    "df_cleaned.drop_duplicates(inplace=True)\n",
    "df_cleaned.dropna(subset=[\"product_name\", \"final_price\"], inplace=True)\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(df[\"colors\"].head(3))\n",
    "print(df_cleaned[df_cleaned[\"product_name\"].str.contains(\"curtain\", case=False, na=False)][[\"product_name\", \"colors\"]])\n",
    "\n",
    "# Save cleaned data\n",
    "df_cleaned.to_csv(r\"C:\\Users\\kriti\\Desktop\\walmart-chatbot\\useful3.csv\", index=False)\n",
    "print(\"‚úÖ Cleaned dataset saved as 'cleaned_walmart_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c45ed80-9c78-4e12-98c4-d596c05cc53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(r\"C:\\Users\\kriti\\Desktop\\walmart-chatbot\\train_expanded.json\" , lines=True)\n",
    "df.to_csv(r'C:\\Users\\kriti\\Desktop\\walmart-chatbot\\train_expanded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a89eb4a-226a-406c-b4c5-a3823a904c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Amex)",
   "language": "python",
   "name": "amex_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
